{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from custom_detect import SingleFrameDetector\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS = r\"G:\\My Drive\\rat_detection\\yolov7_weights\\best.pt\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 640\n",
    "VIDEO = r\"G:\\My Drive\\rat_detection\\active_training_videos\\4Mice.avi\"\n",
    "single_frame_detector = SingleFrameDetector(DEVICE, WEIGHTS, IMAGE_SIZE)\n",
    "frame_path = r\"D:\\cours\\Session Automne 2022\\Vision\\Projet\\yolov7\\problematic_frame.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mG:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMy Drive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mrat_detection\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtest_frames\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m*.jpg\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      4\u001b[0m     frame \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(filename)\n\u001b[1;32m----> 5\u001b[0m     predictions \u001b[39m=\u001b[39m single_frame_detector\u001b[39m.\u001b[39;49mdetect_frame(frame, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(predictions))\n",
      "File \u001b[1;32md:\\cours\\Session Automne 2022\\Vision\\Projet\\yolov7\\custom_detect.py:69\u001b[0m, in \u001b[0;36mSingleFrameDetector.detect_frame\u001b[1;34m(self, frame, conf_thres, iou_thres, classes, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():   \u001b[39m# Calculating gradients would cause a GPU memory leak\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg, augment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     71\u001b[0m \u001b[39m# Apply NMS\u001b[39;00m\n\u001b[0;32m     72\u001b[0m pred \u001b[39m=\u001b[39m non_max_suppression(pred, conf_thres, iou_thres, classes\u001b[39m=\u001b[39mclasses, agnostic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\cours\\Session Automne 2022\\Vision\\Projet\\yolov7\\models\\yolo.py:589\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[39mfor\u001b[39;00m si, fi \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(s, f):\n\u001b[0;32m    588\u001b[0m     xi \u001b[39m=\u001b[39m scale_img(x\u001b[39m.\u001b[39mflip(fi) \u001b[39mif\u001b[39;00m fi \u001b[39melse\u001b[39;00m x, si, gs\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride\u001b[39m.\u001b[39mmax()))\n\u001b[1;32m--> 589\u001b[0m     yi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(xi)[\u001b[39m0\u001b[39m]  \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[39m# cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     yi[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m4\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m si  \u001b[39m# de-scale\u001b[39;00m\n",
      "File \u001b[1;32md:\\cours\\Session Automne 2022\\Vision\\Projet\\yolov7\\models\\yolo.py:625\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    622\u001b[0m         dt\u001b[39m.\u001b[39mappend((time_synchronized() \u001b[39m-\u001b[39m t) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m    623\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m%10.1f\u001b[39;00m\u001b[39m%10.0f\u001b[39;00m\u001b[39m%10.1f\u001b[39;00m\u001b[39mms \u001b[39m\u001b[39m%-40s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (o, m\u001b[39m.\u001b[39mnp, dt[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], m\u001b[39m.\u001b[39mtype))\n\u001b[1;32m--> 625\u001b[0m     x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39mif\u001b[39;00m profile:\n",
      "File \u001b[1;32mc:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\cours\\Session Automne 2022\\Vision\\Projet\\yolov7\\models\\common.py:111\u001b[0m, in \u001b[0;36mConv.fuseforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuseforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[1;32mc:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:394\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 394\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\vince\\.conda\\envs\\py310\\lib\\site-packages\\torch\\nn\\functional.py:2031\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2029\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, inplace\u001b[39m=\u001b[39minplace)\n\u001b[0;32m   2030\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2031\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49msilu_(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   2032\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msilu(\u001b[39minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for filename in glob.glob(r'G:\\My Drive\\rat_detection\\test_frames\\*.jpg'):\n",
    "    frame = cv2.imread(filename)\n",
    "    predictions = single_frame_detector.detect_frame(frame, verbose=False)\n",
    "    print(len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "810554086c624983da14c9423875e2d93c04e2011e93e87fa60d45fc6b058618"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
